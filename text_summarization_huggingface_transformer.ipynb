{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarization_huggingface_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AQBleKxg1CzeUWa457lSZ4Y7grFOSnM_",
      "authorship_tag": "ABX9TyPTfChsuWVNG+ycW4pVWRoM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saifullah3711/text_summarizer_hugging_face/blob/main/text_summarization_huggingface_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the Dependencies"
      ],
      "metadata": {
        "id": "7YbYmITaXtlI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MD73hc_Xcvo",
        "outputId": "e337918a-ef88-4bd9-cc6d-b7cfdd16d935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (2.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20220524)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.1.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (37.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.7/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install PyPDF2\n",
        "!pip install pdfminer.six\n",
        "!pip install fpdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "0Nu1qCxhYcD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "from pdfminer.high_level import extract_text\n",
        "import resource\n",
        "import re\n",
        "import textwrap\n",
        "from fpdf import FPDF"
      ],
      "metadata": {
        "id": "OPxg9iQCXm77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "Ri7nmqOJXjpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual Functions for summarization"
      ],
      "metadata": {
        "id": "NUhn13O4MTlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function tweak the text before saving in the pdf\n",
        "def prep_b4_save(text):\n",
        "  text = re.sub('Gods', 'God\\'s', text)\n",
        "  text = re.sub('yours', 'your\\'s', text)\n",
        "  text = re.sub('dont', 'don\\'t', text)\n",
        "  text = re.sub('doesnt', 'doesn\\'t', text)\n",
        "  text = re.sub('isnt', 'isn\\'t', text)\n",
        "  text = re.sub('havent', 'haven\\'t', text)\n",
        "  text = re.sub('hasnt', 'hasn\\'t', text)\n",
        "  text = re.sub('wouldnt', 'wouldn\\'t', text)\n",
        "  text = re.sub('theyre', 'they\\'re', text)\n",
        "  text = re.sub('youve', 'you\\'ve', text)\n",
        "  text = re.sub('arent', 'aren\\'t', text)\n",
        "  text = re.sub('youre', 'you\\'re', text)\n",
        "  text = re.sub('cant', 'can\\'t', text)\n",
        "  text = re.sub('whore', 'who\\'re', text)\n",
        "  text = re.sub('whos', 'who\\'s', text)\n",
        "  text = re.sub('whatre', 'what\\'re', text)\n",
        "  text = re.sub('whats', 'what\\'s', text)\n",
        "  text = re.sub('hadnt', 'hadn\\'t', text)\n",
        "  text = re.sub('didnt', 'didn\\'t', text)\n",
        "  text = re.sub('couldnt', 'couldn\\'t', text)\n",
        "  text = re.sub('theyll', 'they\\'ll', text)\n",
        "  text = re.sub('youd', 'you\\'d', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "eAkAHEXxxQIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function convert the text into the pdf and save it at the specified location\n",
        "def text_to_pdf(text, filename):\n",
        "    a4_width_mm = 200\n",
        "    pt_to_mm = 0.35\n",
        "    fontsize_pt = 11\n",
        "    fontsize_mm = fontsize_pt * pt_to_mm\n",
        "    margin_bottom_mm = 10\n",
        "    character_width_mm = 7 * pt_to_mm\n",
        "    width_text = a4_width_mm / character_width_mm\n",
        "\n",
        "    pdf = FPDF(orientation='P', unit='mm', format='A4')\n",
        "    pdf.set_auto_page_break(True, margin=margin_bottom_mm)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(family='Courier', size=fontsize_pt)\n",
        "    splitted = text.split('\\n')\n",
        "\n",
        "    for line in splitted:\n",
        "        lines = textwrap.wrap(line, width_text)\n",
        "\n",
        "        if len(lines) == 0:\n",
        "            pdf.ln()\n",
        "\n",
        "        for wrap in lines:\n",
        "            pdf.cell(0, fontsize_mm, wrap, ln=1)\n",
        "\n",
        "    pdf.output(filename, 'F')\n",
        "    print(\"PDF of summary Saved!!\")"
      ],
      "metadata": {
        "id": "UqdCDtH1LT4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function split a huge corpus of text into small chunks or portions\n",
        "def text_chunking(new_text):\n",
        "  max_chunk = 500\n",
        "  new_text = new_text.replace('.', '.<eos>')\n",
        "  new_text = new_text.replace('?', '?<eos>')\n",
        "  new_text = new_text.replace('!', '!<eos>')\n",
        "\n",
        "  sentences = new_text.split('<eos>')\n",
        "  current_chunk = 0 \n",
        "  chunks = []\n",
        "  for sentence in sentences:\n",
        "      if len(chunks) == current_chunk + 1: \n",
        "          if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
        "              chunks[current_chunk].extend(sentence.split(' '))\n",
        "          else:\n",
        "              current_chunk += 1\n",
        "              chunks.append(sentence.split(' '))\n",
        "      else:\n",
        "          # print(current_chunk)\n",
        "          chunks.append(sentence.split(' '))\n",
        "\n",
        "  for chunk_id in range(len(chunks)):\n",
        "    chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
        "  print(\"Total chunks of text are: \", len(chunks))\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "tZAz85V5XkNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes in all the chunks, find the summary of each chunk and return all the summaries of chunks in list form. \n",
        "def model_summary(chunks):\n",
        "  print(\"Summarizing the text. Please wait .......\")\n",
        "  all_summaries = []\n",
        "  count = 0\n",
        "  for chunk in chunks:\n",
        "    print(\"Summarizing Chunk NO: \", count + 1)\n",
        "    res = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
        "    all_summaries +=res\n",
        "    count +=1\n",
        "  return all_summaries\n"
      ],
      "metadata": {
        "id": "F4j4HhwWvald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining all the individual parts into a single function\n",
        "* Input to this function is path to the pdf\n",
        "* This function do all the pre-processing, get the summary and save it in the pdf\n",
        "* Parameter to this function is only the path to the pdf"
      ],
      "metadata": {
        "id": "NUHOI1cvqaZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_summary(pdf_path):\n",
        "  raw_text = extract_text(pdf_path)  # Extract text from the path of pdf given\n",
        "  chunks = text_chunking(raw_text)   # chunk the large text into small parts so it can be supplied to the model\n",
        "  all_summaries = model_summary(chunks) # passing the chunks to the model for the summarization\n",
        "  joined_summary = ' '.join([summ['summary_text'] for summ in all_summaries])  # combine all chunks of summaries to single\n",
        "  txt_to_save = (joined_summary.encode('latin1','ignore')).decode(\"latin1\")  # This ignore the \"aphostrope\" which is little problematic\n",
        "  txt_to_save_prep = prep_b4_save(txt_to_save)\n",
        "  spl = pdf_path.split('/') # Splitting the path based on \"/\" to get the name of the book or pdf\n",
        "  file_name = spl[-1][:-4]+\"_summary.pdf\" # Summary is added at the end i.e book name is the_alchemist so it becomes -> the_alchemist_summary.pdf etc. \n",
        "  text_to_pdf(txt_to_save_prep, file_name)"
      ],
      "metadata": {
        "id": "wWlciGs0qs_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path_alchemist = \"/content/drive/MyDrive/NLP_Projects/text_summarization_NLP_Hugging_face_transformer/the_alchemist.pdf\"\n",
        "find_summary(pdf_path_alchemist)"
      ],
      "metadata": {
        "id": "c7pbRSvp2B9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path_forty = \"/content/forty_rules_of_love.pdf\"\n",
        "find_summary(pdf_path_forty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qm3W4l2JqH",
        "outputId": "eb1e13b7-bc72-4d84-fe76-455673838afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks of text are:  262\n",
            "Summarizing the text. Please wait .......\n",
            "Summarizing Chunk NO:  1\n",
            "Summarizing Chunk NO:  2\n",
            "Summarizing Chunk NO:  3\n",
            "Summarizing Chunk NO:  4\n",
            "Summarizing Chunk NO:  5\n",
            "Summarizing Chunk NO:  6\n",
            "Summarizing Chunk NO:  7\n",
            "Summarizing Chunk NO:  8\n",
            "Summarizing Chunk NO:  9\n",
            "Summarizing Chunk NO:  10\n",
            "Summarizing Chunk NO:  11\n",
            "Summarizing Chunk NO:  12\n",
            "Summarizing Chunk NO:  13\n",
            "Summarizing Chunk NO:  14\n",
            "Summarizing Chunk NO:  15\n",
            "Summarizing Chunk NO:  16\n",
            "Summarizing Chunk NO:  17\n",
            "Summarizing Chunk NO:  18\n",
            "Summarizing Chunk NO:  19\n",
            "Summarizing Chunk NO:  20\n",
            "Summarizing Chunk NO:  21\n",
            "Summarizing Chunk NO:  22\n",
            "Summarizing Chunk NO:  23\n",
            "Summarizing Chunk NO:  24\n",
            "Summarizing Chunk NO:  25\n",
            "Summarizing Chunk NO:  26\n",
            "Summarizing Chunk NO:  27\n",
            "Summarizing Chunk NO:  28\n",
            "Summarizing Chunk NO:  29\n",
            "Summarizing Chunk NO:  30\n",
            "Summarizing Chunk NO:  31\n",
            "Summarizing Chunk NO:  32\n",
            "Summarizing Chunk NO:  33\n",
            "Summarizing Chunk NO:  34\n",
            "Summarizing Chunk NO:  35\n",
            "Summarizing Chunk NO:  36\n",
            "Summarizing Chunk NO:  37\n",
            "Summarizing Chunk NO:  38\n",
            "Summarizing Chunk NO:  39\n",
            "Summarizing Chunk NO:  40\n",
            "Summarizing Chunk NO:  41\n",
            "Summarizing Chunk NO:  42\n",
            "Summarizing Chunk NO:  43\n",
            "Summarizing Chunk NO:  44\n",
            "Summarizing Chunk NO:  45\n",
            "Summarizing Chunk NO:  46\n",
            "Summarizing Chunk NO:  47\n",
            "Summarizing Chunk NO:  48\n",
            "Summarizing Chunk NO:  49\n",
            "Summarizing Chunk NO:  50\n",
            "Summarizing Chunk NO:  51\n",
            "Summarizing Chunk NO:  52\n",
            "Summarizing Chunk NO:  53\n",
            "Summarizing Chunk NO:  54\n",
            "Summarizing Chunk NO:  55\n",
            "Summarizing Chunk NO:  56\n",
            "Summarizing Chunk NO:  57\n",
            "Summarizing Chunk NO:  58\n",
            "Summarizing Chunk NO:  59\n",
            "Summarizing Chunk NO:  60\n",
            "Summarizing Chunk NO:  61\n",
            "Summarizing Chunk NO:  62\n",
            "Summarizing Chunk NO:  63\n",
            "Summarizing Chunk NO:  64\n",
            "Summarizing Chunk NO:  65\n",
            "Summarizing Chunk NO:  66\n",
            "Summarizing Chunk NO:  67\n",
            "Summarizing Chunk NO:  68\n",
            "Summarizing Chunk NO:  69\n",
            "Summarizing Chunk NO:  70\n",
            "Summarizing Chunk NO:  71\n",
            "Summarizing Chunk NO:  72\n",
            "Summarizing Chunk NO:  73\n",
            "Summarizing Chunk NO:  74\n",
            "Summarizing Chunk NO:  75\n",
            "Summarizing Chunk NO:  76\n",
            "Summarizing Chunk NO:  77\n",
            "Summarizing Chunk NO:  78\n",
            "Summarizing Chunk NO:  79\n",
            "Summarizing Chunk NO:  80\n",
            "Summarizing Chunk NO:  81\n",
            "Summarizing Chunk NO:  82\n",
            "Summarizing Chunk NO:  83\n",
            "Summarizing Chunk NO:  84\n",
            "Summarizing Chunk NO:  85\n",
            "Summarizing Chunk NO:  86\n",
            "Summarizing Chunk NO:  87\n",
            "Summarizing Chunk NO:  88\n",
            "Summarizing Chunk NO:  89\n",
            "Summarizing Chunk NO:  90\n",
            "Summarizing Chunk NO:  91\n",
            "Summarizing Chunk NO:  92\n",
            "Summarizing Chunk NO:  93\n",
            "Summarizing Chunk NO:  94\n",
            "Summarizing Chunk NO:  95\n",
            "Summarizing Chunk NO:  96\n",
            "Summarizing Chunk NO:  97\n",
            "Summarizing Chunk NO:  98\n",
            "Summarizing Chunk NO:  99\n",
            "Summarizing Chunk NO:  100\n",
            "Summarizing Chunk NO:  101\n",
            "Summarizing Chunk NO:  102\n",
            "Summarizing Chunk NO:  103\n",
            "Summarizing Chunk NO:  104\n",
            "Summarizing Chunk NO:  105\n",
            "Summarizing Chunk NO:  106\n",
            "Summarizing Chunk NO:  107\n",
            "Summarizing Chunk NO:  108\n",
            "Summarizing Chunk NO:  109\n",
            "Summarizing Chunk NO:  110\n",
            "Summarizing Chunk NO:  111\n",
            "Summarizing Chunk NO:  112\n",
            "Summarizing Chunk NO:  113\n",
            "Summarizing Chunk NO:  114\n",
            "Summarizing Chunk NO:  115\n",
            "Summarizing Chunk NO:  116\n",
            "Summarizing Chunk NO:  117\n",
            "Summarizing Chunk NO:  118\n",
            "Summarizing Chunk NO:  119\n",
            "Summarizing Chunk NO:  120\n",
            "Summarizing Chunk NO:  121\n",
            "Summarizing Chunk NO:  122\n",
            "Summarizing Chunk NO:  123\n",
            "Summarizing Chunk NO:  124\n",
            "Summarizing Chunk NO:  125\n",
            "Summarizing Chunk NO:  126\n",
            "Summarizing Chunk NO:  127\n",
            "Summarizing Chunk NO:  128\n",
            "Summarizing Chunk NO:  129\n",
            "Summarizing Chunk NO:  130\n",
            "Summarizing Chunk NO:  131\n",
            "Summarizing Chunk NO:  132\n",
            "Summarizing Chunk NO:  133\n",
            "Summarizing Chunk NO:  134\n",
            "Summarizing Chunk NO:  135\n",
            "Summarizing Chunk NO:  136\n",
            "Summarizing Chunk NO:  137\n",
            "Summarizing Chunk NO:  138\n",
            "Summarizing Chunk NO:  139\n",
            "Summarizing Chunk NO:  140\n",
            "Summarizing Chunk NO:  141\n",
            "Summarizing Chunk NO:  142\n",
            "Summarizing Chunk NO:  143\n",
            "Summarizing Chunk NO:  144\n",
            "Summarizing Chunk NO:  145\n",
            "Summarizing Chunk NO:  146\n",
            "Summarizing Chunk NO:  147\n",
            "Summarizing Chunk NO:  148\n",
            "Summarizing Chunk NO:  149\n",
            "Summarizing Chunk NO:  150\n",
            "Summarizing Chunk NO:  151\n",
            "Summarizing Chunk NO:  152\n",
            "Summarizing Chunk NO:  153\n",
            "Summarizing Chunk NO:  154\n",
            "Summarizing Chunk NO:  155\n",
            "Summarizing Chunk NO:  156\n",
            "Summarizing Chunk NO:  157\n",
            "Summarizing Chunk NO:  158\n",
            "Summarizing Chunk NO:  159\n",
            "Summarizing Chunk NO:  160\n",
            "Summarizing Chunk NO:  161\n",
            "Summarizing Chunk NO:  162\n",
            "Summarizing Chunk NO:  163\n",
            "Summarizing Chunk NO:  164\n",
            "Summarizing Chunk NO:  165\n",
            "Summarizing Chunk NO:  166\n",
            "Summarizing Chunk NO:  167\n",
            "Summarizing Chunk NO:  168\n",
            "Summarizing Chunk NO:  169\n",
            "Summarizing Chunk NO:  170\n",
            "Summarizing Chunk NO:  171\n",
            "Summarizing Chunk NO:  172\n",
            "Summarizing Chunk NO:  173\n",
            "Summarizing Chunk NO:  174\n",
            "Summarizing Chunk NO:  175\n",
            "Summarizing Chunk NO:  176\n",
            "Summarizing Chunk NO:  177\n",
            "Summarizing Chunk NO:  178\n",
            "Summarizing Chunk NO:  179\n",
            "Summarizing Chunk NO:  180\n",
            "Summarizing Chunk NO:  181\n",
            "Summarizing Chunk NO:  182\n",
            "Summarizing Chunk NO:  183\n",
            "Summarizing Chunk NO:  184\n",
            "Summarizing Chunk NO:  185\n",
            "Summarizing Chunk NO:  186\n",
            "Summarizing Chunk NO:  187\n",
            "Summarizing Chunk NO:  188\n",
            "Summarizing Chunk NO:  189\n",
            "Summarizing Chunk NO:  190\n",
            "Summarizing Chunk NO:  191\n",
            "Summarizing Chunk NO:  192\n",
            "Summarizing Chunk NO:  193\n",
            "Summarizing Chunk NO:  194\n",
            "Summarizing Chunk NO:  195\n",
            "Summarizing Chunk NO:  196\n",
            "Summarizing Chunk NO:  197\n",
            "Summarizing Chunk NO:  198\n",
            "Summarizing Chunk NO:  199\n",
            "Summarizing Chunk NO:  200\n",
            "Summarizing Chunk NO:  201\n",
            "Summarizing Chunk NO:  202\n",
            "Summarizing Chunk NO:  203\n",
            "Summarizing Chunk NO:  204\n",
            "Summarizing Chunk NO:  205\n",
            "Summarizing Chunk NO:  206\n",
            "Summarizing Chunk NO:  207\n",
            "Summarizing Chunk NO:  208\n",
            "Summarizing Chunk NO:  209\n",
            "Summarizing Chunk NO:  210\n",
            "Summarizing Chunk NO:  211\n",
            "Summarizing Chunk NO:  212\n",
            "Summarizing Chunk NO:  213\n",
            "Summarizing Chunk NO:  214\n",
            "Summarizing Chunk NO:  215\n",
            "Summarizing Chunk NO:  216\n",
            "Summarizing Chunk NO:  217\n",
            "Summarizing Chunk NO:  218\n",
            "Summarizing Chunk NO:  219\n",
            "Summarizing Chunk NO:  220\n",
            "Summarizing Chunk NO:  221\n",
            "Summarizing Chunk NO:  222\n",
            "Summarizing Chunk NO:  223\n",
            "Summarizing Chunk NO:  224\n",
            "Summarizing Chunk NO:  225\n",
            "Summarizing Chunk NO:  226\n",
            "Summarizing Chunk NO:  227\n",
            "Summarizing Chunk NO:  228\n",
            "Summarizing Chunk NO:  229\n",
            "Summarizing Chunk NO:  230\n",
            "Summarizing Chunk NO:  231\n",
            "Summarizing Chunk NO:  232\n",
            "Summarizing Chunk NO:  233\n",
            "Summarizing Chunk NO:  234\n",
            "Summarizing Chunk NO:  235\n",
            "Summarizing Chunk NO:  236\n",
            "Summarizing Chunk NO:  237\n",
            "Summarizing Chunk NO:  238\n",
            "Summarizing Chunk NO:  239\n",
            "Summarizing Chunk NO:  240\n",
            "Summarizing Chunk NO:  241\n",
            "Summarizing Chunk NO:  242\n",
            "Summarizing Chunk NO:  243\n",
            "Summarizing Chunk NO:  244\n",
            "Summarizing Chunk NO:  245\n",
            "Summarizing Chunk NO:  246\n",
            "Summarizing Chunk NO:  247\n",
            "Summarizing Chunk NO:  248\n",
            "Summarizing Chunk NO:  249\n",
            "Summarizing Chunk NO:  250\n",
            "Summarizing Chunk NO:  251\n",
            "Summarizing Chunk NO:  252\n",
            "Summarizing Chunk NO:  253\n",
            "Summarizing Chunk NO:  254\n",
            "Summarizing Chunk NO:  255\n",
            "Summarizing Chunk NO:  256\n",
            "Summarizing Chunk NO:  257\n",
            "Summarizing Chunk NO:  258\n",
            "Summarizing Chunk NO:  259\n",
            "Summarizing Chunk NO:  260\n",
            "Summarizing Chunk NO:  261\n",
            "Summarizing Chunk NO:  262\n",
            "PDF of summary Saved!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CDYpgXYsCJIm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}